
<br />

<p align="center">
  <img src="./misc/alstm-logo-yellow.svg" alt="alstm-logo" width="550px" />
</p>

<p align="center">  
  <a href="https://www.python.org/">
    <img src="https://img.shields.io/badge/Python%203.11-3776AB?style=for-the-badge&logo=python&logoColor=yellow&color=3776AB" alt="python-badge" />
  </a>
  <a href="https://www.tensorflow.org/">
    <img src="https://img.shields.io/badge/TensorFlow%202.14-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white" alt="tensoflow-badge" />
  </a>
  <a href="https://www.ibm.com/products/cloudant">
    <img src="https://img.shields.io/badge/IBM%20Cloudant-1261FE?style=for-the-badge&logo=IBM%20Cloud&logoColor=white" alt="python-badge" />
  </a>
</p>

<br />

<p align="center">
Projeto desenvolvido como <b>Trabalho de Conclus√£o de Curso</b> durante o √∫ltimo ano de gradua√ß√£o em <b>Engenharia Mecatr√¥nica</b> na Escola Polit√©cnica da Universidade de S√£o Paulo
</p>

<br />

## üîé Sum√°rio

WIP

Neste reposit√≥rio:

* <a href="#-sobre">üìú Sobre</a> - Breve apresenta√ß√£o do projeto

* <a href="#-rede">üß† Rede</a> - Arquitetura implementada

* <a href="#-treinamento">ü¶æ Treinamento</a> - Otimiza√ß√µes feitas e dados utilizados

* <a href="#-treinamento">üìà Resultados</a> - O que alcan√ßamos com o modelo

* <a href="#-aplica√ß√£o">üåé Aplica√ß√£o</a> - Divulga√ß√£o aberta dos resultados

* <a href="#-colaboradores">ü§ù Colaboradores</a> - Partes envolvidas

## üìú Sobre

> *History never repeats itself, but it does often rhyme*  
> [Mark Twain](https://pt.wikipedia.org/wiki/Mark_Twain)

Este reposit√≥rio cont√©m o c√≥digo do estudo sobre a **previs√£o do √≠ndice [S&P 500](https://www.spglobal.com/spdji/en/indices/equity/sp-500/#overview)**, no qual foi desenvolvido um modelo utilizando **c√©lulas de mem√≥ria de longo-curto prazo ([LSTM](https://en.wikipedia.org/wiki/Long_short-term_memory)) combinadas com [mecanismos de aten√ß√£o](https://en.wikipedia.org/wiki/Attention_(machine_learning))**. O modelo passou por otimiza√ß√µes com t√©cnicas de [*Grid Search* e *Bayesian Seach*](https://en.wikipedia.org/wiki/Hyperparameter_optimization), demonstrando um desempenho promissor na previs√£o de pre√ßos de fechamento.

O trabalho foi majoritariamente inspirado no artigo *[Forecasting stock prices with long-short term memory neural network based on attention mechanism](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0227222)* (2020) de Jiayu Qiu, Bin Wang e Changjun Zhou, recebendo honras como um dos cinco melhores projetos de 2023 no curso.

No estudo tamb√©m foram exploradas aplica√ß√µes mais pr√°ticas do modelo atrav√©s de t√©cnicas de gest√£o de banca, avaliando a rentabilidade das previs√µes em um ambiente controlado e com resultados igualmente promissores.

#### üìï **[Vers√£o final da monografia](https://www.github.com/gvmossato/alstm-stock-market/misc/Previs√£o_do_√çndice_SP_500_Utilizando_LSTM_e_Mecanismos_de_Atencao.pdf)**

## Problem√°tica & Motiva√ß√£o

WIP

## Dados & Pre-Processamento

Fluxograma simplificado dos dados sendo processados at√© serem consumidos pelo modelo:

![dados](https://i.ibb.co/tmhPnPk/dados.png)

Conforme a imagem:

1. **Fonte:** o conjunto de dados utilizado nesse projeto √© um recorte hist√≥rico abrangendo de 03 de janeiro de 1983 a 01 de setembro de 2023, retirado diretamente do [Yahoo! Finance](https://finance.yahoo.com/quote/%5EGSPC/).

2. **Redu√ß√£o de Ru√≠do:** [transformada wavelet](https://towardsdatascience.com/the-wavelet-transform-e9cfa85d7b34) com a fam√≠lia de fun√ß√µes Coiflets at√© terceira ordem, a qual √© particularmente eficaz na redu√ß√£o de ru√≠do em sinais n√£o estacion√°rios como os de pre√ßos de a√ß√µes.

3. **Normaliza√ß√£o:** para assegurar que todas as vari√°veis tenham o mesmo peso durante o treinamento da rede, aplicamos a normaliza√ß√£o [Z-Score](https://www.statology.org/z-score-normalization/). Isso coloca todas as vari√°veis na mesma escala, neutralizando o efeito de disparidades nas magnitudes de pre√ßos e volumes transacionais.

4. **Reparti√ß√£o:** os dados s√£o divididos em tr√™s segmentos: treino, valida√ß√£o e teste, na propor√ß√£o de 95/2.5/2.5. Escolhemos essa divis√£o para permitir um ajuste fino dos hiperpar√¢metros (usando o conjunto de valida√ß√£o) e uma avalia√ß√£o honesta do desempenho do modelo (usando o conjunto de teste). Ao manter a ordem temporal (sem embaralhamento), respeitamos a sequ√™ncia natural dos eventos no mercado de a√ß√µes.

5. **Janelamento:** implementamos uma t√©cnica de janela deslizante com tamanho de 20 dias (aproximadamente um m√™s em dias √∫teis), que constitui o passo temporal do nosso modelo. Dentro dessa janela, seis s√©ries temporais distintas ‚Äî abertura, fechamento, m√°xima, m√≠nima, [fechamento ajustado](https://help.yahoo.com/kb/SLN28256.html) e volume (em quantidades) ‚Äî s√£o fornecidas ao modelo. Com esses dados ele prediz o valor de fechamento no vig√©simo primeiro dia.

## ü¶æ Otimiza√ß√£o

Como citado, realizamos a tunagem de hiperpar√¢metros em duas fases distintas:

### Grid Search

Explorar deterministicamente as redondezas do modelo apresentado no artigo base utilizado.

- **Tamanho do Estado Oculto**: valores de 10, 20, 50 e 100 para o tamanho da mem√≥ria das c√©lulas LSTM.
- **Taxa de Aprendizado**: experimentamos com 0,001, 0,01 e 0,1 para entender o impacto na converg√™ncia do modelo.
- **Tamanho do Lote**: variamos entre 64, 128, 256 e 512 para observar as diferen√ßas no treinamento do modelo.

Um total de **48 combina√ß√µes √∫nicas** de hiperpar√¢metros foram avaliadas, utilizando a metodologia de [valida√ß√£o cruzada com k-dobras](https://medium.com/@soumyachess1496/cross-validation-in-time-series-566ae4981ce4) (k=3), resultando em **144 execu√ß√µes distintas**.

### Bayesian Search

Na sequ√™ncia, um Bayesian Search mais refinado foi implementado com base nas combina√ß√µes de hiperpar√¢metros mais promissoras do Grid Search.

- **Tamanho do Lote (Batch Size)**: valores espec√≠ficos de 64, 128, 256, 512 e 1024 foram considerados.
- **Taxa de Aprendizado (Learning Rate)**: um intervalo cont√≠nuo de 0,0001 a 0,01 foi explorado.
- **Taxa de Dropout**: avaliamos um espectro cont√≠nuo de 0% a 30% para encontrar o equil√≠brio ideal na preven√ß√£o de overfitting.

Decidimos fixar o **tamanho do estado oculto** em 20, baseando-nos nas descobertas do Grid Search e na sintonia com o tamanho de entrada de dados (20 dias √∫teis).

Ao longo de **100 configura√ß√µes**, avaliamos e validamos diferentes modelos, totalizando **300 execu√ß√µes** por meio da valida√ß√£o cruzada com k-dobras (k=3). Essa abordagem mais exaustiva foi desenhada para identificar a combina√ß√£o ideal de hiperpar√¢metros que aprimoraria a performance do modelo.

### Resumo

Consolidado dos testes:

| Hiperpar√¢metro | Grid Search | Bayesian Search |
|----------------|-------------|-----------------|
| Tamanho do Estado Oculto | 10, 20, 50, 100 | N√£o testado |
| Taxa de Aprendizado | 0,001; 0,01; 0,1 | 0,0001 a 0,01 |
| Tamanho do Lote | 64, 128, 256, 512 | 64, 128, 256, 512, 1024 |
| Taxa de Dropout | N√£o testado | 0% a 30% |

## üß† Rede & Treinamento

A arquitetura proposta para o modelo de previs√£o do √≠ndice S&P 500 incorpora um total de **20 c√©lulas LSTM**, em conson√¢ncia com a janela de 20 dias que √© analisada. Isto √©, para uma janela m√≥vel de 20 dias, s√£o lidos [OHLC](https://www.investopedia.com/terms/o/ohlcchart.asp) + Fechamento Ajustado + Volume (quantidades), para ent√£o predizer o fechamento do 21¬∫ dia.

![modelo](https://i.ibb.co/jk7DVzR/modelo.png)

Como msotra imagem anterior, ap√≥s o processamento pelas c√©lulas LSTM, as sa√≠das s√£o submetidas ao mecanismo de *[soft attention](https://stackoverflow.com/questions/35549588/soft-attention-vs-hard-attention)*. Esse mecanismo avalia as contribui√ß√µes de cada c√©lula LSTM e pondera sua influ√™ncia, permitindo que a import√¢ncia de momentos distintos no tempo seja diferenciada, ao inv√©s de focar apenas na informa√ß√£o mais recente. Isso se baseia na premissa de que eventos passados dentro da janela de tempo podem ter relev√¢ncia semelhante ou at√© maior do que os mais recentes.

A "camada de aten√ß√£o" ent√£o agrega as sa√≠das das c√©lulas LSTM ponderadas em um vetor de contexto que concentra as informa√ß√µes relevantes detectadas pela rede. Esse vetor de contexto √© ent√£o passado por uma camada de [dropout](https://towardsdatascience.com/dropout-in-neural-networks-47a162d621d9#waht-is-a-dropout), com uma taxa de desativa√ß√£o de 12,241%, antes de ser apresentado √† √∫ltima camada da rede.

A fase final da arquitetura √© composta por uma camada densa com um √∫nico neur√¥nio, cuja **fun√ß√£o de ativa√ß√£o linear** √© adequada para tarefas de regress√£o como a previs√£o de √≠ndices de a√ß√µes. Esse neur√¥nio processa o vetor e produz o output final da rede: a previs√£o do valor de fechamento do S&P 500.

Por fim, o treinamento da rede ocorre ao longo de **2000 epochs** com um **batch size de 128**. O algoritmo **[ADAM](https://medium.com/@LayanSA/complete-guide-to-adam-optimization-1e5f29532c3d)** √© o escolhido para otimiza√ß√£o, operando com uma **taxa de aprendizado de 0,00018** - esses par√¢metros foram selecionados com base nos resultados da busca em grid, da busca bayesiana e da an√°lise da curva de aprendizado.

## üìà Resultados

WIP

## üåé Aplica√ß√£o

WIP

## Uso

WIP

## ü§ù Colaboradores

O trabalho foi desenvolvido por mim, [Gabriel Mossato](https://br.linkedin.com/in/gvmossato), em parceria com o √† √©poca tamb√©m graduando [Paulino Fonseca](https://br.linkedin.com/in/paulinoveloso) sob a orienta√ß√£o do [Prof. Dr. Oswaldo Luiz do Valle Costa](https://bv.fapesp.br/en/pesquisador/191/oswaldo-luiz-do-valle-costa) do departamento de Engenharia El√©trica da EP-USP.
