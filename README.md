
<br />

<p align="center">
  <img src="./misc/alstm-logo-yellow.svg" alt="alstm-logo" width="550px" />
</p>

<p align="center">  
  <a href="https://www.python.org/">
    <img src="https://img.shields.io/badge/Python%203.11-3776AB?style=for-the-badge&logo=python&logoColor=yellow&color=3776AB" alt="python-badge" />
  </a>
  <a href="https://www.tensorflow.org/">
    <img src="https://img.shields.io/badge/TensorFlow%202.14-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white" alt="tensoflow-badge" />
  </a>
  <a href="https://www.ibm.com/products/cloudant">
    <img src="https://img.shields.io/badge/IBM%20Cloudant-1261FE?style=for-the-badge&logo=IBM%20Cloud&logoColor=white" alt="python-badge" />
  </a>
</p>

<br />

<p align="center">
Projeto desenvolvido como <b>Trabalho de ConclusÃ£o de Curso</b> durante o Ãºltimo ano de graduaÃ§Ã£o em <b>Engenharia MecatrÃ´nica</b> na Escola PolitÃ©cnica da Universidade de SÃ£o Paulo (EP-USP)
</p>

<br />

## ğŸ” SumÃ¡rio

Neste repositÃ³rio:

* <a href="#-sobre">ğŸ“œ Sobre</a> - Breve apresentaÃ§Ã£o

* <a href="#-problemÃ¡tica--motivaÃ§Ã£o">â‰ï¸ ProblemÃ¡tica & MotivaÃ§Ã£o</a> - PorquÃªs e objetivos

* <a href="#-dados--pre-processamento">âš™ï¸ Dados & PrÃ©-Processamento</a> - Pipeline de dados

* <a href="#-otimizaÃ§Ã£o">ğŸ¦¾ OtimizaÃ§Ã£o</a> - Ajuste dos hiperparÃ¢metros

* <a href="#-rede--treinamento">ğŸ§  Rede & Treinamento</a> - Arquitetura implementada

* <a href="#-resultados">ğŸ“ˆ Resultados</a> - Desempenho do modelo

* <a href="#-aplicaÃ§Ã£o">ğŸŒ AplicaÃ§Ã£o</a> - VisualizaÃ§Ã£o prÃ¡tica

* <a href="#-uso--cÃ³digo">ğŸ‘¨â€ğŸ’» Uso & CÃ³digo</a> - OrientaÃ§Ãµes gerais

* <a href="#-colaboradores">ğŸ¤ Colaboradores</a> - Equipe envolvida


## ğŸ“œ Sobre

> *History never repeats itself, but it does often rhyme*  
> [Mark Twain](https://pt.wikipedia.org/wiki/Mark_Twain)

Este repositÃ³rio contÃ©m o cÃ³digo do estudo sobre a **previsÃ£o do Ã­ndice S&P 500**, no qual foi desenvolvido um modelo utilizando **cÃ©lulas de memÃ³ria de longo-curto prazo (LSTM) combinadas com mecanismos de atenÃ§Ã£o**. O modelo passou por otimizaÃ§Ãµes com tÃ©cnicas de Grid Search e Bayesian Seach, demonstrando um desempenho promissor na previsÃ£o de preÃ§os de fechamento.

O trabalho foi majoritariamente inspirado no artigo **[Forecasting stock prices with long-short term memory neural network based on attention mechanism](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0227222)** (2020) de Jiayu Qiu, Bin Wang e Changjun Zhou, recebendo honras como um dos cinco melhores projetos de 2023 no curso.

No estudo tambÃ©m foram exploradas aplicaÃ§Ãµes mais prÃ¡ticas do modelo atravÃ©s de tÃ©cnicas de gestÃ£o de banca, avaliando a rentabilidade das previsÃµes em um ambiente controlado e com resultados igualmente promissores.

#### ğŸ“• **[VersÃ£o final da monografia](https://github.com/gvmossato/alstm-stock-market/blob/main/misc/Previs%C3%A3o%20do%20%C3%8Dndice%20S%26P%20500%20Utilizando%20LSTM%20e%20Mecanismos%20de%20Aten%C3%A7%C3%A3o.pdf)**

## â‰ï¸ ProblemÃ¡tica & MotivaÃ§Ã£o

O aprofundamento financeiro do Brasil, marcado pela liberalizaÃ§Ã£o, desregulamentaÃ§Ã£o e inovaÃ§Ã£o, catalisou um aumento expressivo no nÃºmero de participantes do mercado financeiro, incluindo tambÃ©m pequenas gestoras e investidores individuais. Entretanto, esse crescimento acompanha tambÃ©m uma ascensÃ£o da desinformaÃ§Ã£o, exacerbada pela vulnerabilidade Ã  fraude e pela difusÃ£o de anÃ¡lises de fontes nÃ£o especializadas.

<p align="center"> 
  <img src="https://i.ibb.co/BjvJbqq/decision-making-process.png" alt="decision-making-process" height="190px" />
  <span>Â Â Â Â Â Â Â </span>
  <img src="https://i.ibb.co/XkNtbC2/fraud.png" alt="fraud" height="190px" />
</p>

Em paralelo, entende-se que a economia global Ã©, de certo modo, refletido pelo Ã­ndice S&P 500 devido Ã  vasta operaÃ§Ã£o internacional das empresas listadas nesse. Tal cenÃ¡rio abre espaÃ§o para que modelos preditivos possam empoderar o processo de tomada de decisÃ£o de investimentos, especialmente para aqueles recÃ©m-chegados ao mercado.

Assim, a motivaÃ§Ã£o desse trabalho reside na necessidade de fortalecer o processo decisÃ³rio no contexto complexo e volÃ¡til do mercado de aÃ§Ãµes. O modelo proposto busca ser uma ferramenta auxiliar, nÃ£o substituindo, mas sim potencializando o raciocÃ­nio estratÃ©gico de investidores. Para tal, os recentes desenvolvimentos em mecanismos de atenÃ§Ã£o em redes neurais sÃ£o aplicados aqui Ã s sÃ©ries temporais financeiras oferecendo aos investidores, especialmente aqueles com recursos limitados e acesso tardio a informaÃ§Ãµes, uma fonte a mais para suas decisÃµes.

## âš™ï¸ Dados & PrÃ©-Processamento

Fluxograma simplificado do prÃ©-processamento dos dados atÃ© serem consumidos pelo modelo:

![dados](https://i.ibb.co/tmhPnPk/dados.png)

Conforme a imagem:

1. **Fonte:** o conjunto de dados utilizado nesse projeto Ã© um recorte histÃ³rico abrangendo de 03 de janeiro de 1983 a 01 de setembro de 2023, retirado do Yahoo! Finance.

2. **ReduÃ§Ã£o de RuÃ­do:** [transformada wavelet](https://towardsdatascience.com/the-wavelet-transform-e9cfa85d7b34) com a famÃ­lia de funÃ§Ãµes Coiflets atÃ© terceira ordem, a qual Ã© particularmente eficaz na reduÃ§Ã£o de ruÃ­do em sinais nÃ£o estacionÃ¡rios como os de preÃ§os de aÃ§Ãµes.

3. **NormalizaÃ§Ã£o:** para assegurar que todas as variÃ¡veis tenham o mesmo peso durante o treinamento da rede, aplicamos a normalizaÃ§Ã£o [Z-Score](https://www.statology.org/z-score-normalization/). Isso coloca todas as variÃ¡veis na mesma escala, neutralizando o efeito de disparidades nas magnitudes de preÃ§os e volumes transacionais.

4. **RepartiÃ§Ã£o:** os dados sÃ£o divididos em trÃªs segmentos: treino, validaÃ§Ã£o e teste, na proporÃ§Ã£o de 95/2.5/2.5. Escolhemos essa divisÃ£o para permitir um ajuste fino dos hiperparÃ¢metros (usando o conjunto de validaÃ§Ã£o) e uma avaliaÃ§Ã£o honesta do desempenho do modelo (usando o conjunto de teste). Ao manter a ordem temporal (sem embaralhamento), respeitamos a sequÃªncia natural dos eventos no mercado de aÃ§Ãµes.

5. **Janelamento:** implementamos uma tÃ©cnica de janela deslizante com tamanho de 20 dias (aproximadamente um mÃªs em dias Ãºteis), que constitui o passo temporal do nosso modelo. Dentro dessa janela, seis sÃ©ries temporais distintas â€” abertura, fechamento, mÃ¡xima, mÃ­nima, [fechamento ajustado](https://help.yahoo.com/kb/SLN28256.html) e volume (em quantidades) â€” sÃ£o fornecidas ao modelo. Com esses dados ele prediz o valor de fechamento no vigÃ©simo primeiro dia.

## ğŸ¦¾ OtimizaÃ§Ã£o

Como citado, realizamos a tunagem de hiperparÃ¢metros em duas fases distintas: um Grid Search e Bayesian Search. No primeiro, buscamos explorar deterministicamente as redondezas do modelo apresentado no artigo base utilizado. Assim, com um total de **48 combinaÃ§Ãµes Ãºnicas** de hiperparÃ¢metros avaliadas utilizando a metodologia de [validaÃ§Ã£o cruzada com k-dobras](https://medium.com/@soumyachess1496/cross-validation-in-time-series-566ae4981ce4) obtivemos **144 execuÃ§Ãµes distintas** (k=3).

Na sequÃªncia, o Bayesian Search foi implementado com base nas combinaÃ§Ãµes de hiperparÃ¢metros mais promissoras do Grid Search, em uma tentativa de refinar a rede. Nesse ponto, decidimos fixar o tamanho do estado oculto em 20, baseando-nos nas descobertas do Grid Search e na sintonia com o tamanho de entrada de dados (20 dias Ãºteis). Ao longo de **100 configuraÃ§Ãµes** de redes, avaliamos e validamos diferentes modelos, totalizando **300 execuÃ§Ãµes**.

Um resumo dos testes encontra-se na tabela abaixo:

| HiperparÃ¢metro | Grid Search | Bayesian Search |
|----------------|-------------|-----------------|
| Tamanho do Estado Oculto | 10, 20, 50, 100 | NÃ£o testado |
| Taxa de Aprendizado | 0,001; 0,01; 0,1 | 0,0001 a 0,01 |
| Tamanho do Lote | 64, 128, 256, 512 | 64, 128, 256, 512, 1024 |
| Taxa de Dropout | NÃ£o testado | 0% a 30% |

## ğŸ§  Rede & Treinamento

A arquitetura proposta para o modelo de previsÃ£o do Ã­ndice S&P 500 incorpora um total de **20 cÃ©lulas LSTM**, em consonÃ¢ncia com a janela de 20 dias que Ã© analisada. Isto Ã©, para uma janela mÃ³vel de 20 dias, sÃ£o lidos [OHLC](https://www.investopedia.com/terms/o/ohlcchart.asp) + Fechamento Ajustado + Volume (quantidades), para entÃ£o predizer o fechamento do 21Âº dia.

![modelo](https://i.ibb.co/jk7DVzR/modelo.png)

Como msotra imagem anterior, apÃ³s o processamento pelas cÃ©lulas LSTM, as saÃ­das sÃ£o submetidas ao mecanismo de *[soft attention](https://stackoverflow.com/questions/35549588/soft-attention-vs-hard-attention)*. Esse mecanismo avalia as contribuiÃ§Ãµes de cada cÃ©lula LSTM e pondera sua influÃªncia, permitindo que a importÃ¢ncia de momentos distintos no tempo seja diferenciada, ao invÃ©s de focar apenas na informaÃ§Ã£o mais recente. Isso se baseia na premissa de que eventos passados dentro da janela de tempo podem ter relevÃ¢ncia semelhante ou atÃ© maior do que os mais recentes.

A "camada de atenÃ§Ã£o" entÃ£o agrega as saÃ­das das cÃ©lulas LSTM ponderadas em um vetor de contexto que concentra as informaÃ§Ãµes relevantes detectadas pela rede. Esse vetor de contexto Ã© entÃ£o passado por uma camada de [dropout](https://towardsdatascience.com/dropout-in-neural-networks-47a162d621d9#waht-is-a-dropout), com uma taxa de desativaÃ§Ã£o de 12,241%, antes de ser apresentado Ã  Ãºltima camada da rede.

A fase final da arquitetura Ã© composta por uma camada densa com um Ãºnico neurÃ´nio, cuja **funÃ§Ã£o de ativaÃ§Ã£o linear** Ã© adequada para tarefas de regressÃ£o como a previsÃ£o de Ã­ndices de aÃ§Ãµes. Esse neurÃ´nio processa o vetor e produz o output final da rede: a previsÃ£o do valor de fechamento do S&P 500.

Por fim, o treinamento da rede ocorre ao longo de **2000 epochs** com um **batch size de 128**. O algoritmo **[ADAM](https://medium.com/@LayanSA/complete-guide-to-adam-optimization-1e5f29532c3d)** Ã© o escolhido para otimizaÃ§Ã£o, operando com uma **taxa de aprendizado de 0,00018** - esses parÃ¢metros foram selecionados com base nos resultados da busca em grid, da busca bayesiana e da anÃ¡lise da curva de aprendizado.

## ğŸ“ˆ Resultados

A seÃ§Ã£o de resultados se prolonga por algumas dezenas de pÃ¡ginas da monografia, entÃ£o, nÃ£o sendo pertinente trazer todos os resultados, abordamos aqui um recorte conveniente do que alcanÃ§amos com o modelo.

Qualitativamente, os resultados no conjunto de teste e o retorno acumulado ao longo do perÃ­odo sÃ£o bastante fidedignos ao que se observou no mercado para Ã  Ã©poca:

<p align="center">
  <img src="https://i.ibb.co/71mF1gB/test-results.png" alt="test-results" width="350px" />
  <span>Â Â Â Â Â Â Â </span>
  <img src="https://i.ibb.co/mhkQhh6/return-results.png" alt="return-results" width="350px" />
</p>

Optamos ainda por fazer um estudo comparativo do mecanismo de atenÃ§Ã£o, testando variaÃ§Ãµes desse: um rede sem atenÃ§Ã£o, uma rede com a atenÃ§Ã£o como proposta no artigo de referÃªncia (benchmark) e a "atenÃ§Ã£o clÃ¡ssica", proposta no artigo inaugural [Attention Is All You Need](https://arxiv.org/abs/1706.03762):

<p align="center">
  <img src="https://i.ibb.co/TkDq633/attention-results.png" alt="attention-results" width="900px" />
</p>

Ã‰ observÃ¡vel, portanto, a influÃªncia e consequente melhora do desempenho do modelo com o uso do mecanismo clÃ¡ssico. Quando passamos, todavia, a avaliar quantitativamente o modelo frente ao benchmark, encontramos um problema inicial:

|                  | Benchmark    | Modelo         |
|------------------|--------------|----------------|
| Data mÃ­nima (teste) | 2019-05-17   | 2022-09-23  |
| Data mÃ¡xima (teste) | 2019-07-01   | 2023-09-01  |
| ObservaÃ§Ãµes        | 31           | 237          |
| PreÃ§o MÃ­nimo       | 2751,53      | 3577,03      |
| PreÃ§o MÃ¡ximo       | 2971,41      | 4588,96      |
| Amplitude de PreÃ§os| 219,88       | 1011,93      |
| PreÃ§o MÃ©dio        | 2870,27      | 4095,33      |
| Volatilidade Anualizada | 11,28%  | 17,58%       |

Como sintetiza a tabela anterior, o conjunto de teste do nosso modelo encontrava-se em um mercado muito mais complexo: mais observaÃ§Ãµes com maiores amplitudes de preÃ§o e uma volatilidade notadamente superior. Assim, a comparaÃ§Ã£o direta nÃ£o poderia ser considerada justa, optamos entÃ£o por normalizar a comparaÃ§Ã£o das mÃ©tricas de erro por dois mÃ©todos, preÃ§o mÃ©dio e amplitude:

|                        | Benchmark    | Modelo       |
|------------------------|--------------|--------------|
| **MÃ©tricas**           |              |              |
| RMSE                   | 0,3475       | 19,5238      |
| MAE                    | 0,1935       | 13,9011      |
| $R^2$                  | 0,8783       | 0,9940       |
| **NormalizaÃ§Ã£o pelo PreÃ§o MÃ©dio** |   |              |
| RMSE                   | 0,00012107   | 0,00476734   |
| MAE                    | 0,00006742   | 0,00339437   |
| **NormalizaÃ§Ã£o pela Amplitude de PreÃ§os** | |        |
| RMSE                   | 0,00158041   | 0,01929363   |
| MAE                    | 0,00088003   | 0,01373717   |

Todavia, mesmo apÃ³s a normalizaÃ§Ã£o, com exceÃ§Ã£o do $R^2$, nÃ£o superamos o benchmark. NÃ£o obstante, esses resultados forneceram um *insight* valioso: embora nÃ£o estejamos acertando adequadamente o preÃ§o de fechamento, estamos fazendo uma leitura muito satisfatÃ³ria da tendÃªncia. Ora, vamos entÃ£o aprofundar essa anÃ¡lise:

<p align="center">
  <img src="https://i.ibb.co/C77TdZZ/trend-results.png" alt="trend-results" width="350px" />
  <span>Â Â Â Â Â Â Â </span>
  <img src="https://i.ibb.co/Fz7XPkn/confusion-matrix.png" alt="confusion-matrix" width="350px" />
</p>

Constatamos que o modelo de fato parece seguir muito bem as oscilaÃ§Ãµes, prevendo com consistÃªncia quando o mercado irÃ¡ subir ou cair:

* Dado que o Ã­ndice subiu, acertamos **84,72%** das vezes.
* Dado que o Ã­ndice caiu, acertamos **80,43%** das vezes.

Frente a esses resultados, optamos entÃ£o por tentar validar o modelo em uma abordagem um pouco mais prÃ¡tica, donde surge a iniciativa de aplicar estratÃ©gias de gestÃ£o de banca para operar no mercado tendo em vista as previsÃµes. Foram exploradas diversas estratÃ©gias (Martingale, Paroli, D'Alembert, etc.) em um ambiente simulado simplificado, cujas hipÃ³teses foram:

1. Livre de custos
2. Liquidez e volume suficientes no mercado
3. OperaÃ§Ãµes ao preÃ§o de fechamento
4. Sem alavancagem
5. **Compra e *short selling* sÃ£o igualmente complexos**

Destarte, mediante a previsÃ£o do modelo e a estratÃ©gia de gestÃ£o escolhida, o investidor entra comprado ou vendido no ativo, ganhando ou perdendo consoante a variaÃ§Ã£o do Ã­ndice no perÃ­odo. Os resultados para uma das estratÃ©gias mais rentÃ¡veis â€” Paroli â€” pode ser visto no grÃ¡fico abaixo:

<p align="center">
  <img src="https://i.ibb.co/0KZDTdk/paroli-results.png" alt="paroli-results" width="600px" />
</p>

Finalmente, como um todo, o quadro de resultados da gestÃ£o de banca fica expresso por:

<p align="center">
  <img src="https://i.ibb.co/DYxjYSs/bet-results.png" alt="bet-results" width="700px" />
</p>

## ğŸŒ AplicaÃ§Ã£o

Como um todo, a implementaÃ§Ã£o do projeto pode ser segmentada entre duas grandes frentes: o **modelo**, que compreende basicamente a tudo que fora exposto atÃ© aqui, como a rede, o treinamento, as validaÃ§Ãµes tÃ©cnicas e prÃ¡ticas, etc. Em paralelo, existe ainda a **aplicaÃ§Ã£o**: uma plataforma web, hospedada em um outro [repositÃ³rio dedicado](https://github.com/gvmossato/alstm-front), para proporcionar uma interface intuitiva o suficiente a fim de permitir que usuÃ¡rios comuns pudessem usufruir das prediÃ§Ãµes do modelo sem conhecimento tÃ©cnico em programaÃ§Ã£o.

A aplicaÃ§Ã£o permaneceu operante atÃ© meados de abril de 2024, sendo incorporados os dados mais recentes disponÃ­veis Ã  Ã©poca a cada **seis meses**, em treinamentos incrementais automÃ¡ticos com **15 epochs** e integraÃ§Ã£o ao [IBM Cloudant](https://www.ibm.com/br-pt/products/cloudant), provedor do banco de dados NoSQL utilizado.

## ğŸ‘¨â€ğŸ’» Uso & CÃ³digo

Para executar o **modelo** e definir opÃ§Ãµes especÃ­ficas de ajuste de parÃ¢metros e carregamento de pesos de sessÃµes de treinamento anteriores (se necessÃ¡rio), utilize:

```css
poetry run model [-t {grid,bayes}] [-w]
```

ParÃ¢metros opcionais:

* `-t`, `--tuning`: especifica o mÃ©todo de otimizaÃ§Ã£o a ser executado. Se nÃ£o especificado, o ajuste de parÃ¢metros nÃ£o serÃ¡ realizado. As configuraÃ§Ãµes para cada tipo de ajuste devem ser definidas diretamente no cÃ³digo. Aceita:

  * `grid`: utiliza o Grid Search para otimizar os parÃ¢metros.

  * `bayes`: utiliza o Bayesian Search para otimizar os parÃ¢metros.

* `-w`, `--load-weights`: carrega os pesos salvos da sessÃ£o de treinamento mais recente.

  * PadrÃ£o: `False` (nÃ£o carrega os pesos automaticamente).

<br />

Para executar a **aplicaÃ§Ã£o** para realizar previsÃµes com os dados mais recentes disponÃ­veis, sincronizaÃ§Ã£o com a nuvem e treinamentos incrementais (se necessÃ¡rio), utilize:

```
poetry run app
```

Note que para integraÃ§Ã£o com o banco de dados serÃ¡ necessÃ¡rio especificar as varÃ¡veis de ambiente requeridas pelo serviÃ§o de nuvem: `DATABASE`, `CLOUDANT_USERNAME`, `CLOUDANT_PASSWORD` e `CLOUDANT_HOST`.

***

Em relaÃ§Ã£o ao cÃ³digo, a Ã¡rvore de arquivos do projeto estÃ¡ organizada como:

```
ğŸ“¦alstm_stock_market
 â”£ ğŸ“‚images
 â”£ ğŸ“‚logs
 â”£ ğŸ“‚src
 â”ƒ â”£ ğŸ“‚app
 â”ƒ â”ƒ â”— ğŸ“œapp.py
 â”ƒ â”£ ğŸ“‚data
 â”ƒ â”ƒ â”— ğŸ“œpreprocessor.py
 â”ƒ â”£ ğŸ“‚helpers
 â”ƒ â”ƒ â”£ ğŸ“‚calendars
 â”ƒ â”ƒ â”ƒ â”— ğŸ“œus.cal
 â”ƒ â”ƒ â”£ ğŸ“œplotter.py
 â”ƒ â”ƒ â”— ğŸ“œutils.py
 â”ƒ â”£ ğŸ“‚manager
 â”ƒ â”ƒ â”£ ğŸ“œmanager.py
 â”ƒ â”ƒ â”— ğŸ“œstrategies.py
 â”ƒ â”£ ğŸ“‚model
 â”ƒ â”ƒ â”£ ğŸ“‚weights
 â”ƒ â”ƒ â”£ ğŸ“œevaluator.py
 â”ƒ â”ƒ â”£ ğŸ“œmodel.py
 â”ƒ â”ƒ â”— ğŸ“œparams.py
 â”— ğŸ“œrun.py
```

Ela estÃ¡ dividida em mÃ³dulos que concentram as distintas operaÃ§Ãµes do cÃ³digo:

* `ğŸ“‚src/model/`: contÃ©m os arquivos referentes ao modelo em si, como arquitetura, hiperparÃ¢metros da rede e mÃ©tricas de avaliaÃ§Ã£o. Na subpasta `weights` encontram-se os arquivos `.h5` com os pesos do modelo apÃ³s treinamentos.

* `ğŸ“‚src/app/`: contÃ©m a lÃ³gica que permite ao modelo ser executado em produÃ§Ã£o, conforme trabalhado na seÃ§Ã£o "AplicaÃ§Ã£o". As rotinas de treinamentos incrementais e comunicaÃ§Ã£o com a nuvem (IBM Cloudant) encontram-se aqui.

* `ğŸ“‚src/manager/`: contÃ©m os arquivos referentes Ã  gestÃ£o de banca. SubdivisÃ£o do cÃ³digo implementada para avaliar o desempenho do modelo em um cenÃ¡rio ainda controlado, mas mais prÃ³ximo da prÃ¡tica, operando com distintas estratÃ©gias frente Ã s previsÃµes.

* `ğŸ“‚src/data/`: contÃ©m os arquivos referentes a todo o pipeline de dados exposto, capaz de lidar com cada um dos casos de uso esperados (treinamento inicial, treinamentos adicionais, uso em produÃ§Ã£o, etc.).

* `ğŸ“‚src/helpers/`: contÃ©m os arquivos gerais e de uso compartilhado entre os demais mÃ³dulos, funÃ§Ãµes e mÃ©todos auxiliares.

<br />

Por fim, a pasta `images` Ã© utilizada para salvar os plots em formatados vetorizados, se desejÃ¡vel, enquanto `logs` armazena registros de execuÃ§Ã£o do mÃ³dulo de aplicaÃ§Ã£o.

## ğŸ¤ Colaboradores

Este projeto foi desenvolvido por [Gabriel Mossato](https://br.linkedin.com/in/gvmossato) em colaboraÃ§Ã£o com [Paulino Fonseca](https://br.linkedin.com/in/paulinoveloso), ambos Ã  Ã©poca graduandos sob orientaÃ§Ã£o do [Prof. Dr. Oswaldo Luiz do Valle Costa](https://bv.fapesp.br/en/pesquisador/191/oswaldo-luiz-do-valle-costa), pertencente ao Departamento de Engenharia ElÃ©trica da Escola PolitÃ©cnica da Universidade de SÃ£o Paulo (EP-USP).
